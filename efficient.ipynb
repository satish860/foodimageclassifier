{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8065357-c3c1-4f59-beb5-72497e42293f",
   "metadata": {},
   "source": [
    "# Food Image Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc0d95b-8149-438f-add6-380776c4e03c",
   "metadata": {},
   "source": [
    "This part of the Manning Live project - https://liveproject.manning.com/project/210 . In synposis, By working on this project, I will be classying the food variety of 101 type. Dataset is already availble in public but we will be starting with subset of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed12a2a-5909-4630-9673-166a11516a08",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbfd6e-760c-4b69-8d9f-62e96ef3693a",
   "metadata": {},
   "source": [
    "As a general best practice to ALWAYS start with a subset of the dataset rather than a full one. There are two reason for the same\n",
    "1. As you experiement with the model, You dont want to run over all the dataset that will slow down the process\n",
    "2. You will end up wasting lots of GPU resources well before the getting best model for the Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c60d40-ea8d-4631-8513-9806eec89b39",
   "metadata": {},
   "source": [
    "In the Case live Project, The authors already shared the subset of the notebook so we can use the same for the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f239b39-206f-4db1-b6be-61d8f2bef6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://lp-prod-resources.s3-us-west-2.amazonaws.com/other/Deploying+a+Deep+Learning+Model+on+Web+and+Mobile+Applications+Using+TensorFlow/Food+101+-+Data+Subset.zip\n",
    "#!unzip Food+101+-+Data+Subset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c5034b-5222-4657-ba06-81a01d0b362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install efficientnet_pytorch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3e2ca9d-d036-4cdf-af27-41c442406481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets,models\n",
    "import torchvision.transforms as tt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader,random_split,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7131c77a-bd58-469f-bb78-257be94bf798",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "train_tfms = tt.Compose([tt.RandomHorizontalFlip(),\n",
    "                         tt.Resize([224,224]),\n",
    "                         tt.ToTensor(), \n",
    "                         tt.Normalize(*stats,inplace=True)])\n",
    "valid_tfms = tt.Compose([tt.Resize([224,224]),tt.ToTensor(), tt.Normalize(*stats)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b39a9d8-5ff7-4346-be90-f5300b42e7dd",
   "metadata": {},
   "source": [
    "Create a Pytorch dataset from the image folder. This will allow us to create a Training dataset and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cf472cb-4c94-42b5-8eac-8c6654d6ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.ImageFolder('food-101-subset/images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f63aa79b-56a7-4b75-a33e-6d6e313e200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,ds,transformer):\n",
    "        self.ds = ds\n",
    "        self.transform = transformer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image,label = self.ds[idx]\n",
    "        img = self.transform(image)\n",
    "        return img,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7515e326-8044-4e12-a5de-1bf16f54eeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 600)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len=0.8*len(ds)\n",
    "val_len = len(ds) - train_len\n",
    "int(train_len),int(val_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e72938b5-1b58-4351-a16f-7576efc4b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,val_ds = random_split(dataset=ds,lengths=[int(train_len),int(val_len)],generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0526f6d8-b787-4786-addc-ab9a36686c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ds = CustomDataset(train_ds.dataset,train_tfms)\n",
    "v_ds = CustomDataset(val_ds.dataset,valid_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89419b9b-2f5a-4560-b57c-38db76dfc831",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dl = DataLoader(t_ds, batch_size, shuffle=True, pin_memory=True)\n",
    "valid_dl = DataLoader(v_ds, batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a594d5f4-a33f-46ef-8bc4-e0d397d5eca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for x,yb in train_dl:\n",
    "    print(x.shape)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71f0d08d-5e56-4a14-9f4c-260e11f88be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761c2aa-c191-4d92-85a6-bb7b329f6519",
   "metadata": {},
   "source": [
    "# Create a ResNet Model with default Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15c84715-cd76-498f-99d4-2f7acf240d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
    "head = nn.Sequential(nn.Dropout(0.2),nn.Flatten(),nn.Linear(1280,100),nn.Linear(100,3),MemoryEfficientSwish())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5a68295-0a98-4ae4-b227-bf821a2cabc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0991, grad_fn=<NllLossBackward>)\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "los = nn.CrossEntropyLoss()\n",
    "for x,y in train_dl:\n",
    "    f = body.extract_features(x)\n",
    "    f = nn.functional.adaptive_avg_pool2d(f, (1, 1))\n",
    "    f = torch.flatten(f, 1) \n",
    "    yb = head(f)\n",
    "    loss = los(yb,y)\n",
    "    print(loss)\n",
    "    print(y.shape)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1fa05cb-3ea7-402b-a53c-c6ac410a3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch.utils import MemoryEfficientSwish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5cf32fcc-3d6c-4cae-af42-718df3e6f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return torch.flatten(x,1)\n",
    "\n",
    "class FoodImageClassifer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
    "        self.head = nn.Sequential(nn.Dropout(0.2),nn.Linear(1280,3),MemoryEfficientSwish())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.model.extract_features(x)\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x, 1) \n",
    "        return self.head(x)\n",
    "    \n",
    "    def freeze(self):\n",
    "        return true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9be02918-0e9b-47f5-ba53-aa851cea5d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs,model,train_dl,valid_dl,loss_fn,opt):\n",
    "    mb = master_bar(range(epochs))\n",
    "    mb.write(['epoch','train_loss','valid_loss','trn_acc','val_acc'],table=True)\n",
    "\n",
    "    for i in mb:    \n",
    "        trn_loss,val_loss = 0.0,0.0\n",
    "        trn_acc,val_acc = 0,0\n",
    "        trn_n,val_n = len(train_dl.dataset),len(valid_dl.dataset)\n",
    "        model.train()\n",
    "        for xb,yb in progress_bar(train_dl,parent=mb):\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(out,yb)\n",
    "            _,pred = torch.max(out.data, 1)\n",
    "            trn_acc += (pred == yb).sum().item()\n",
    "            trn_loss += loss.item()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        trn_loss /= mb.child.total\n",
    "        trn_acc /= trn_n\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in progress_bar(valid_dl,parent=mb):\n",
    "                xb,yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                loss = loss_fn(out,yb)\n",
    "                val_loss += loss.item()\n",
    "                _,pred = torch.max(out.data, 1)\n",
    "                val_acc += (pred == yb).sum().item()\n",
    "        val_loss /= mb.child.total\n",
    "        val_acc /= val_n\n",
    "\n",
    "        mb.write([i,f'{trn_loss:.6f}',f'{val_loss:.6f}',f'{trn_acc:.6f}',f'{val_acc:.6f}'],table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ccfda2-cf03-4aa7-a8c8-a3d6ab90d138",
   "metadata": {},
   "source": [
    "# Making the Resnet as a Feature Extractor and training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e6803-a37b-4961-b0db-26fd7a61dd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='9' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      90.00% [9/10 07:12<00:48]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>trn_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.070354</td>\n",
       "      <td>1.024411</td>\n",
       "      <td>0.513667</td>\n",
       "      <td>0.712333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.974153</td>\n",
       "      <td>0.847737</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.824333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.749302</td>\n",
       "      <td>0.522657</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.859667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.489167</td>\n",
       "      <td>0.337034</td>\n",
       "      <td>0.851000</td>\n",
       "      <td>0.890333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.344713</td>\n",
       "      <td>0.252785</td>\n",
       "      <td>0.882333</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.279971</td>\n",
       "      <td>0.195080</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>0.937333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.235730</td>\n",
       "      <td>0.159489</td>\n",
       "      <td>0.923000</td>\n",
       "      <td>0.949333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.195900</td>\n",
       "      <td>0.128502</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.957667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.171641</td>\n",
       "      <td>0.107370</td>\n",
       "      <td>0.937667</td>\n",
       "      <td>0.965667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='23' class='' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      24.47% [23/94 00:05<00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = FoodImageClassifer()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "#model.freeze()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "fit(10,model=model,train_dl=train_dl,valid_dl=valid_dl,loss_fn=criterion,opt=optimizer_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bc42d5-5e82-4d36-b46b-a674badf5725",
   "metadata": {},
   "source": [
    "# Freeze the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f82dd-5029-4c74-936e-5594fe0ae70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict,'efficient.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844fb78-5627-4b6e-a692-b252b331f051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
