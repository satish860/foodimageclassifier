{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8065357-c3c1-4f59-beb5-72497e42293f",
   "metadata": {},
   "source": [
    "# Food Image Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc0d95b-8149-438f-add6-380776c4e03c",
   "metadata": {},
   "source": [
    "This part of the Manning Live project - https://liveproject.manning.com/project/210 . In synposis, By working on this project, I will be classying the food variety of 101 type. Dataset is already availble in public but we will be starting with subset of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed12a2a-5909-4630-9673-166a11516a08",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbfd6e-760c-4b69-8d9f-62e96ef3693a",
   "metadata": {},
   "source": [
    "As a general best practice to ALWAYS start with a subset of the dataset rather than a full one. There are two reason for the same\n",
    "1. As you experiement with the model, You dont want to run over all the dataset that will slow down the process\n",
    "2. You will end up wasting lots of GPU resources well before the getting best model for the Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c60d40-ea8d-4631-8513-9806eec89b39",
   "metadata": {},
   "source": [
    "In the Case live Project, The authors already shared the subset of the notebook so we can use the same for the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f239b39-206f-4db1-b6be-61d8f2bef6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://lp-prod-resources.s3-us-west-2.amazonaws.com/other/Deploying+a+Deep+Learning+Model+on+Web+and+Mobile+Applications+Using+TensorFlow/Food+101+-+Data+Subset.zip\n",
    "#!unzip Food+101+-+Data+Subset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e2ca9d-d036-4cdf-af27-41c442406481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets,models\n",
    "import torchvision.transforms as tt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader,random_split,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from fastprogress.fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7131c77a-bd58-469f-bb78-257be94bf798",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "train_tfms = tt.Compose([tt.Resize(256),\n",
    "                         tt.CenterCrop(224),\n",
    "                         tt.ToTensor(), \n",
    "                         tt.Normalize(*stats,inplace=True)])\n",
    "valid_tfms = tt.Compose([tt.Resize([224,224]),tt.ToTensor(), tt.Normalize(*stats)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b39a9d8-5ff7-4346-be90-f5300b42e7dd",
   "metadata": {},
   "source": [
    "Create a Pytorch dataset from the image folder. This will allow us to create a Training dataset and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5cf472cb-4c94-42b5-8eac-8c6654d6ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.ImageFolder('food-101-subset/images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f63aa79b-56a7-4b75-a33e-6d6e313e200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,ds,transformer):\n",
    "        self.ds = ds\n",
    "        self.transform = transformer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image,label = self.ds[idx]\n",
    "        img = self.transform(image)\n",
    "        return img,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7515e326-8044-4e12-a5de-1bf16f54eeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 600)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len=0.8*len(ds)\n",
    "val_len = len(ds) - train_len\n",
    "int(train_len),int(val_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e72938b5-1b58-4351-a16f-7576efc4b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,val_ds = random_split(dataset=ds,lengths=[int(train_len),int(val_len)],generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0526f6d8-b787-4786-addc-ab9a36686c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ds = CustomDataset(train_ds.dataset,train_tfms)\n",
    "v_ds = CustomDataset(val_ds.dataset,valid_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89419b9b-2f5a-4560-b57c-38db76dfc831",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dl = DataLoader(t_ds, batch_size, shuffle=True, pin_memory=True)\n",
    "valid_dl = DataLoader(v_ds, batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a594d5f4-a33f-46ef-8bc4-e0d397d5eca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for x,yb in train_dl:\n",
    "    print(x.shape)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71f0d08d-5e56-4a14-9f4c-260e11f88be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761c2aa-c191-4d92-85a6-bb7b329f6519",
   "metadata": {},
   "source": [
    "# Create a MobileNet Model with default Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb701280-48c0-4bb2-8d6c-abdffd4d4296",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet = models.mobilenet_v2(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "882b50ac-87d9-4b97-8e87-be229d63a544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1000])\n"
     ]
    }
   ],
   "source": [
    "for xb,yb in train_dl:\n",
    "    x = mobilenet.features(xb)\n",
    "    x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "    x = torch.flatten(x, 1)\n",
    "    classifi = mobilenet.classifier(x)\n",
    "    print(classifi.shape)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5cf32fcc-3d6c-4cae-af42-718df3e6f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return torch.flatten(x,1)\n",
    "\n",
    "class FoodImageClassifer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        self.body = mobilenet.features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(1280,3))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.body(x)\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x, 1) \n",
    "        return self.head(x)\n",
    "    \n",
    "    def freeze(self):\n",
    "        for name,param in self.body.named_parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9be02918-0e9b-47f5-ba53-aa851cea5d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs,model,train_dl,valid_dl,loss_fn,opt):\n",
    "    mb = master_bar(range(epochs))\n",
    "    mb.write(['epoch','train_loss','valid_loss','trn_acc','val_acc'],table=True)\n",
    "\n",
    "    for i in mb:    \n",
    "        trn_loss,val_loss = 0.0,0.0\n",
    "        trn_acc,val_acc = 0,0\n",
    "        trn_n,val_n = len(train_dl.dataset),len(valid_dl.dataset)\n",
    "        model.train()\n",
    "        for xb,yb in progress_bar(train_dl,parent=mb):\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(out,yb)\n",
    "            _,pred = torch.max(out.data, 1)\n",
    "            trn_acc += (pred == yb).sum().item()\n",
    "            trn_loss += loss.item()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        trn_loss /= mb.child.total\n",
    "        trn_acc /= trn_n\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in progress_bar(valid_dl,parent=mb):\n",
    "                xb,yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                loss = loss_fn(out,yb)\n",
    "                val_loss += loss.item()\n",
    "                _,pred = torch.max(out.data, 1)\n",
    "                val_acc += (pred == yb).sum().item()\n",
    "        val_loss /= mb.child.total\n",
    "        val_acc /= val_n\n",
    "\n",
    "        mb.write([i,f'{trn_loss:.6f}',f'{val_loss:.6f}',f'{trn_acc:.6f}',f'{val_acc:.6f}'],table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ccfda2-cf03-4aa7-a8c8-a3d6ab90d138",
   "metadata": {},
   "source": [
    "# Making the Resnet as a Feature Extractor and training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e64e6803-a37b-4961-b0db-26fd7a61dd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>trn_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.454271</td>\n",
       "      <td>0.173601</td>\n",
       "      <td>0.819667</td>\n",
       "      <td>0.941333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.159274</td>\n",
       "      <td>0.107771</td>\n",
       "      <td>0.942333</td>\n",
       "      <td>0.965333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.102381</td>\n",
       "      <td>0.086633</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>0.971333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.063304</td>\n",
       "      <td>0.059722</td>\n",
       "      <td>0.981667</td>\n",
       "      <td>0.981667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.036076</td>\n",
       "      <td>0.051212</td>\n",
       "      <td>0.991333</td>\n",
       "      <td>0.981667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.032023</td>\n",
       "      <td>0.044363</td>\n",
       "      <td>0.991333</td>\n",
       "      <td>0.983667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.021476</td>\n",
       "      <td>0.046935</td>\n",
       "      <td>0.996333</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.034029</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.988667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.013824</td>\n",
       "      <td>0.042734</td>\n",
       "      <td>0.997333</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.011312</td>\n",
       "      <td>0.034711</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.989667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = FoodImageClassifer()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "#model.freeze()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "fit(10,model=model,train_dl=train_dl,valid_dl=valid_dl,loss_fn=criterion,opt=optimizer_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bc42d5-5e82-4d36-b46b-a674badf5725",
   "metadata": {},
   "source": [
    "# Freeze the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c7dd7daa-1612-492b-b369-81347215b281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>trn_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.329174</td>\n",
       "      <td>0.114477</td>\n",
       "      <td>0.880333</td>\n",
       "      <td>0.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.093847</td>\n",
       "      <td>0.044478</td>\n",
       "      <td>0.969333</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.052012</td>\n",
       "      <td>0.042672</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.984667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.029423</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.992333</td>\n",
       "      <td>0.991333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.012145</td>\n",
       "      <td>0.027692</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.989667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.015205</td>\n",
       "      <td>0.036340</td>\n",
       "      <td>0.995667</td>\n",
       "      <td>0.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.031336</td>\n",
       "      <td>0.052772</td>\n",
       "      <td>0.989333</td>\n",
       "      <td>0.980667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>0.037750</td>\n",
       "      <td>0.992333</td>\n",
       "      <td>0.988667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.010021</td>\n",
       "      <td>0.043159</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.008288</td>\n",
       "      <td>0.018306</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.995000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = FoodImageClassifer()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=1e-4)\n",
    "model.freeze()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "fit(10,model=model,train_dl=train_dl,valid_dl=valid_dl,loss_fn=criterion,opt=optimizer_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "501f82dd-5029-4c74-936e-5594fe0ae70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict,'mobilenet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844fb78-5627-4b6e-a692-b252b331f051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
